{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr66haYdE-8s"
      },
      "source": [
        "# LIBS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btHb0atrEAR5",
        "outputId": "7da3d662-dcc4-446e-c082-341eab0b42b8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "## Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "\n",
        "## Librerias para graficación\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLTK es una librería particular para PLN. Tiene muchas funcionalidades entre ellas stemming y lista de palabras de parada.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "stemmer = nltk.stem.SnowballStemmer('english') # Vamos a utlizar el Snowball Stemmer para realizar stemming (nos permite llevar las palabras a una forma estandar).\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bojW8YdE7Sj"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb8bBTHuE7E0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv', sep=',', header=0, index_col= None, engine='python',\n",
        "                 usecols=['text','emotion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsV2jc0KFZ3j"
      },
      "source": [
        "# CLASSIFIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alfKIeT5Fjy8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "* processing_text\n",
        "* @param texto str\n",
        "* @return processed_feature str\n",
        "'''\n",
        "def processing_text(texto):\n",
        "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
        "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
        "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
        "    # Paso 5: Pasar todo el texto a minúsculas\n",
        "    processed_feature = processed_feature.lower()\n",
        "    # Paso 6: Aplicar stemming. Es una forma de enviar las palabras a una raiz común simplificando de esta manera el vocabulario.\n",
        "    #         por ejemplo las palabras (absurdo, absurdos) que estan en el review 2895 seran llevados a la raiz común \"absurd\"\n",
        "    #         y de esta forma se evita tener dos palabras diferentes con el mismo significado en nuestro vocabulario.\n",
        "    processed_feature = \" \".join([stemmer.stem(i) for i in processed_feature.split()])\n",
        "\n",
        "\n",
        "    return processed_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC8-bVLBG2qk"
      },
      "source": [
        "Procesamiento de texto: No contemple números, pase a minúscula, elimine caracteres individuales,\n",
        "tokenize por palabra, y aplique stemming.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_HH5cGGGPVC"
      },
      "outputs": [],
      "source": [
        "# Primero vamos extraer del dataframe la columna texto y la polaridad y las almacenaremos en las variables\n",
        "# texto_para_procesar y labels respectivamente\n",
        "texto_para_procesar = (df['text'].values)\n",
        "labels = df['emotion'].values\n",
        "\n",
        "# El texto ya procesado de cada ejemplo en nuestro dataset lo almacenaremos en la variable \"texto_procesado\"\n",
        "texto_procesado = []\n",
        "for sentence in range(0, len(texto_para_procesar)):\n",
        "    procesado = processing_text(texto_para_procesar[sentence])\n",
        "    texto_procesado.append(procesado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkrpS8TfIus8"
      },
      "source": [
        "Funcion del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seVjUHiAILmL"
      },
      "outputs": [],
      "source": [
        "def create_model(texto_features, model, eta=0.1):\n",
        "    # Partición del dataset: Seleccionar 80% para entrenamiento, 20% pruebas.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        texto_features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Modelo: Naive Bayes | SDGClassifier\n",
        "    if model == \"naives\":\n",
        "        model = MultinomialNB()\n",
        "        model.fit(X_train, y_train)\n",
        "    else:\n",
        "        model = SGDClassifier(\n",
        "            loss='log_loss', learning_rate='constant', eta0=eta)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # Reporte de clasificación\n",
        "    predictions = model.predict(X_test)\n",
        "    print(classification_report(y_test, predictions, digits=4))\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\t\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm, display_labels=model.classes_)\n",
        "    disp.plot()\n",
        "\n",
        "    return model,accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iHeQQp5Fem0"
      },
      "source": [
        "## CLASSIFIERS (NB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkCZaJUsGPvk"
      },
      "source": [
        "### CLASSIFIER 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-eHfITJGv5q"
      },
      "source": [
        "Representación: Bolsa de palabras con un vocabulario de 500 tokens, eliminando stop\n",
        "words(ingles)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PYoaOf-Gq9h"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=500, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model(texto_features, \"naives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xylIsLmJJf1a"
      },
      "source": [
        "### CLASSIFIER 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTuadFEmJkmi"
      },
      "source": [
        "Representación: Bolsa de palabras con un vocabulario de 1000 tokens, eliminando stop\n",
        "words(ingles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "TOBFogFzJjt_",
        "outputId": "8e28cf5e-8dea-4688-9f07-ade87783152a"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model(texto_features, \"naives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX3J_VblJv-9"
      },
      "source": [
        "### CLASSIFIER 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPYNK-wJy5m"
      },
      "source": [
        "Representación: Bolsa de palabras con un vocabulario de 5000 tokens, eliminando stop\n",
        "words(ingles)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e21bUZMUJyAL"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model(texto_features, \"naives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00kQEQQFxFj"
      },
      "source": [
        "## CLASSFIER (SDG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOVpIFIEMnFV"
      },
      "source": [
        "### CLASSIFIER 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "dC0nLEOWF2Br",
        "outputId": "1b553167-65a0-4f01-b4fa-b9aa9d27fd9d"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "model_words,_ = create_model(texto_features, \"sdg\", eta=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3VtAVzGMr_t"
      },
      "source": [
        "### CLASSIFIER 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "kTywkLtuMuWU",
        "outputId": "948c2407-4bee-47f1-84dd-0067591442a1"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model(texto_features, \"sdg\", eta=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrS_84tGNJJK"
      },
      "source": [
        "# RESPUESTAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bjY1rgkNLtJ"
      },
      "source": [
        "## ¿Qué efecto sobre el F1-score y el accuracy tiene el incremento del vocabulario?, es bueno o negativo incrementarlo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66EjwDUyNWnZ"
      },
      "source": [
        "rpta:  `El incremento del vocabulario tuvo cambios positivos en cuanto al porcentaje de f1-score y el accuracy incrementandolo de 89. a 93., sin embargo, hubieron problemas relacionados con la ram al momento de generar la matriz de features para los vocabularios de 5000 palabras, esto se resolvio cambiando el tipo dato para las matrices de float64 a float16`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhS0GvcNQYC"
      },
      "source": [
        "## ¿En base a los resultados del clasificador SGDClassifier y experimentación adicional que realice, para este problema que valor de la tasa de aprendizaje es apropiado?, ¿Vale la pena incrementarlo como en el clasificador 5?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hallando la tasa de aprendizaje apropiado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tasa_acc = None\n",
        "eta = 0.1\n",
        "accuracy = 0\n",
        "for i in range(10):\n",
        "    _,acc = create_model(texto_features, \"sdg\", eta=eta)\n",
        "    if acc >= accuracy:\n",
        "        accuracy = acc\n",
        "        tasa_acc = eta\n",
        "    eta += 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{tasa_acc} - {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hIdgisNUqC"
      },
      "source": [
        "rpta: `Luego de realizar las pruebas cambiando e incrementando la tasa de aprendizaje, obtuve que el mejor resultado es asignandole un valor de 0.1, ya que al incrementar esa tasa el accuracy como el f1-score van disminuyendo`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5351TG8oNXpo"
      },
      "source": [
        "## Los coeficientes que se obtienen del SGD son un indicativo de importancia de características. ¿Utilizando el clasificador 4, cuáles son las palabras más relevantes (importantes) para la tarea de clasificación?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CJfMcEJPZL-"
      },
      "outputs": [],
      "source": [
        "def top_words(palabras, coefs):\n",
        "    words = []\n",
        "    for i in range(len(coefs)):\n",
        "        words.append((palabras[i],coefs[i]))\n",
        "\n",
        "    positives = sorted(words, key=lambda x: (x[1]),reverse = True)\n",
        "    negatives = sorted(words, key=lambda x: (x[1]),reverse = False)\n",
        "    return positives,negatives\n",
        "positives,negatives = top_words(vectorizer.get_feature_names_out(),model_words.coef_[0])\n",
        "top_words = positives[:5] + negatives[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df74u7DlP1Pa",
        "outputId": "6210f317-e9c7-462a-a6ca-fc6895076186"
      },
      "outputs": [],
      "source": [
        "top_words = sorted(top_words, key=lambda x: ( abs(x[1])),reverse = True)\n",
        "top_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ5Shjv1NbWB"
      },
      "source": [
        "rpta: `Tras crear un touple donde guardamos la palabra con su coeficiente (peso|W), le realizamos los ordenamientos en base a su coeficiente y podemos observar las palabras mas importantes tanto negativas como positivas son las siguientes :`\n",
        "\n",
        "`\n",
        "\t[('rotten'),\n",
        "\t('lame'),\n",
        "\t('letharg'),\n",
        "\t('vain'),\n",
        "\t('lousi'),\n",
        "\t('innoc'),\n",
        "\t('superior'),\n",
        "\t('intellig'),\n",
        "\t('divin'),\n",
        "\t('belov')]\n",
        "`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
