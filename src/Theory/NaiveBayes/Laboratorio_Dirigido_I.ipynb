{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27854a96-ff8b-4574-a853-123793fb2219",
      "metadata": {
        "id": "27854a96-ff8b-4574-a853-123793fb2219"
      },
      "source": [
        "# Clasificación de sentimientos: Enfoque de Aprendizaje de Máquina\n",
        "\n",
        "En este notebook, vas a usar [scikit-learn](https://scikit-learn.org/), una de las librerías mas importantes para construcción de modelos de aprendizaje de máquina, para la construcción de un clasificador de sentimientos. El objetivo es identificar si una crítica de una película es positiva o negativa.  Este notebook esta dividido en tres secciones que representan las etapas típicas en la construcción de soluciones de procesamiento de lenguaje natural.\n",
        "\n",
        "- *Procesamiento del texto*: Vamos a entender el dataset y procesar el texto.\n",
        "- *Estrategia de representación*: Construiremos una representación básica de bolsa de palabras utilizando la funcionalidad CountVectorizer de sklearn.\n",
        "- *Modelamiento y evaluación*: Entrenaremos un clasificador Naive Bayes y evaluaremos su desempeño. Tambien utilizaremos el modelo construido para predecir la polaridad en texto construido por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "primary-wiring",
      "metadata": {
        "id": "primary-wiring"
      },
      "outputs": [],
      "source": [
        "### Lo primero que vamos a realizar es la importación de las librerías necesarias para la construcción de nuestro clasificador\n",
        "\n",
        "## Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "\n",
        "## Librerias para graficación\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLTK es una librería particular para PLN. Tiene muchas funcionalidades entre ellas stemming y lista de palabras de parada.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "stemmer = nltk.stem.SnowballStemmer('spanish') # Vamos a utlizar el Snowball Stemmer para realizar stemming (nos permite llevar las palabras a una forma estandar).\n",
        "nltk.download('stopwords') # Lista de palabras de parada en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d64584",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0f05367d-ba85-45fe-937e-4b92032e59d8",
      "metadata": {
        "id": "0f05367d-ba85-45fe-937e-4b92032e59d8"
      },
      "source": [
        "## 1. Procesamiento del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ddea59-fd79-4cef-9b01-1fd1ec9a5f8a",
      "metadata": {
        "id": "53ddea59-fd79-4cef-9b01-1fd1ec9a5f8a"
      },
      "source": [
        "#### 1.1. Dataset\n",
        "\n",
        "Vamos a utlizar una versión modificada del dataset abierto de criticas de películas Españolas de Kaggle: [Críticas películas filmaffinity en Español](https://www.kaggle.com/datasets/ricardomoya/criticas-peliculas-filmaffinity-en-espaniol). El dataset se encuentra en archivo csv \"Tutorial-NLP_Analisis_de_Sentimientos.csv\".\n",
        "\n",
        "Para leer este archivo y operarlo vamos a hacer uso de la librería de python [Pandas](https://pandas.pydata.org/) y su método de lectura [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-projection",
      "metadata": {
        "id": "collected-projection"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Tutorial-NLP_Analisis_de_Sentimientos.csv', sep=',', header=0, index_col= None, engine='python',\n",
        "                 usecols=['film_name','gender','review_title','review_text','polaridad'])\n",
        "\n",
        "## Una vez realizado el cargue del dataset en un dataframe de pandas podemos explorarlo!\n",
        "## Revisemos 5 ejemplos del dataset.\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe7b59e-8808-44c6-bf7a-4c31f1a1815c",
      "metadata": {
        "id": "abe7b59e-8808-44c6-bf7a-4c31f1a1815c"
      },
      "source": [
        "Cada ejemplo del dataset (fila) representa una crítica realizada por un usuario a una pelicula.\n",
        "\n",
        "- La columna '*film_name*' indica el nombre de la película objeto de la crítica.\n",
        "- La columna '*gender*' indica el género de la película.\n",
        "- La columna '*review title*' es el título de la crítica.\n",
        "- La columna '*polaridad*' es la categoría/label asignado a la crítica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eef9640d-a4a3-4d72-9308-8ff10dba9213",
      "metadata": {
        "id": "eef9640d-a4a3-4d72-9308-8ff10dba9213"
      },
      "source": [
        "Revisemos la critica en la fila número 2895 en nuestro dataset. Para acceder vamos a utilizar [iloc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) que nos permite seleccionar por indice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nutritional-artwork",
      "metadata": {
        "id": "nutritional-artwork"
      },
      "outputs": [],
      "source": [
        "df.loc[2895].review_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ecbdc3a-50a3-420b-b011-d867cd0b7b04",
      "metadata": {
        "id": "8ecbdc3a-50a3-420b-b011-d867cd0b7b04"
      },
      "source": [
        "Claramente la crítica es negativa, pero revisemos su categoría/label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regulation-basics",
      "metadata": {
        "id": "regulation-basics"
      },
      "outputs": [],
      "source": [
        "df.loc[2895].polaridad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe2329a-f77f-45dc-ade1-b39180eb08cd",
      "metadata": {
        "id": "4fe2329a-f77f-45dc-ade1-b39180eb08cd"
      },
      "source": [
        "Revisemos ahora cuantos ejemplos de críticas negativas y positivas tenemos en nuestro dataset. Esta información es relevante para:\n",
        "- Seleccionar la relación entre particiones de entrenamiento y testing de nuestro dataset.\n",
        "- Conocer si las clases estas balanceadas (cada clase tiene el mismo número de ejemplos).\n",
        "- Seleccionar le modelo de aprendizaje de máquina mas apropiado.\n",
        "\n",
        "Vamos a utilizar la función [catplot](https://seaborn.pydata.org/generated/seaborn.catplot.html) de la librería seaborn para la elaboración de un histograma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sustainable-somewhere",
      "metadata": {
        "id": "sustainable-somewhere"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='polaridad', kind='count', color='r', data=df)\n",
        "plt.title('Distribución de Ejemplos')\n",
        "plt.xlabel('Polaridad')\n",
        "plt.ylabel('Conteo')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff28a07-75d8-4cf9-8f1c-c8dad3fc1a06",
      "metadata": {
        "id": "2ff28a07-75d8-4cf9-8f1c-c8dad3fc1a06"
      },
      "source": [
        "Del histograma se puede evidenciar:\n",
        "- Tenemos 1857 ejemplos de cada clase (positivo, negativo). Las clases estan perfectamente balanceadas.\n",
        "- En total tenemos 3714 ejemplos en el dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef41c35-4a79-4a4d-81c1-cb6ca96391ad",
      "metadata": {
        "id": "9ef41c35-4a79-4a4d-81c1-cb6ca96391ad"
      },
      "source": [
        "#### 1.2. Unificación de texto\n",
        "\n",
        "En nuestro dataset tenemos dos fuentes de información textual de la crítica la columna review_text y la columna review_title. Vamos a concatenar ambos textos en una sola nueva columna que denomineros \"*texto*\". Usaremos su contenido para construir una representación de bolsa de palabras y posteriormente un modelo de Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recent-console",
      "metadata": {
        "id": "recent-console"
      },
      "outputs": [],
      "source": [
        "df['texto'] = df['review_title'] + ' ' + df['review_text']\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae190f5-9192-4609-81ca-7802fdbaffcf",
      "metadata": {
        "id": "dae190f5-9192-4609-81ca-7802fdbaffcf"
      },
      "source": [
        "#### 1.3. Función de procesamiento\n",
        "\n",
        "Vamos a definir ahora una función \"*processing_text*\" encargada de modificar el texto de forma apropiada para ser usado en la representación. Es importante aclarar que el procesamiento cambia de acuerdo al dataset y el problema que se esta resolviendo, no existe una única formula mágica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "durable-camping",
      "metadata": {
        "id": "durable-camping"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "* processing_text\n",
        "* @param texto str\n",
        "* @return processed_feature str\n",
        "'''\n",
        "def processing_text(texto):\n",
        "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
        "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
        "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
        "    # Paso 5: Pasar todo el texto a minúsculas\n",
        "    processed_feature = processed_feature.lower()\n",
        "    # Paso 6: Aplicar stemming. Es una forma de enviar las palabras a una raiz común simplificando de esta manera el vocabulario.\n",
        "    #         por ejemplo las palabras (absurdo, absurdos) que estan en el review 2895 seran llevados a la raiz común \"absurd\"\n",
        "    #         y de esta forma se evita tener dos palabras diferentes con el mismo significado en nuestro vocabulario.\n",
        "    processed_feature = \" \".join([stemmer.stem(i) for i in processed_feature.split()])\n",
        "\n",
        "\t#\n",
        " \n",
        "    return processed_feature\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fa029d-d65d-4fa5-a427-faa8339d1566",
      "metadata": {
        "id": "61fa029d-d65d-4fa5-a427-faa8339d1566"
      },
      "source": [
        "Ahora que ya tenemos nuestra función de procesamiento la vamos a aplicar a nuestro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "integrated-devices",
      "metadata": {
        "id": "integrated-devices"
      },
      "outputs": [],
      "source": [
        "# Primero vamos extraer del dataframe la columna texto y la polaridad y las almacenaremos en las variables\n",
        "# texto_para_procesar y labels respectivamente\n",
        "texto_para_procesar = df['texto'].values\n",
        "labels = df['polaridad'].values\n",
        "\n",
        "# El texto ya procesado de cada ejemplo en nuestro dataset lo almacenaremos en la variable \"texto_procesado\"\n",
        "texto_procesado = []\n",
        "for sentence in range(0, len(texto_para_procesar)):\n",
        "    procesado = processing_text(texto_para_procesar[sentence])\n",
        "    texto_procesado.append(procesado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "criminal-delivery",
      "metadata": {
        "id": "criminal-delivery"
      },
      "outputs": [],
      "source": [
        "# Comparemos ahora la crítica 2895 procesada vs no procesada.\n",
        "print(\"Sin procesar:\")\n",
        "print(texto_para_procesar[2895])\n",
        "print(\"---------------------------------\")\n",
        "print(\"Procesado:\")\n",
        "print(texto_procesado[2895])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c299ad4-6900-476c-a90a-99228627ebb9",
      "metadata": {
        "id": "0c299ad4-6900-476c-a90a-99228627ebb9"
      },
      "source": [
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "\n",
        "Debido al proceso de stemming ahora el texto es mucho mas dificil de leer, sin embargo, el stemming nos permitirá tener un vocabulario reducido.\n",
        "Ahora solo nos hace falta una etapa del procesamiento que es la eliminación de *palabras de parada*. Las palabras de parada son las palabras más comunes en cualquier idioma:  artículos, preposiciones, pronombres, conjunciones, etc. y no agregan mucha información sobre la semántica del texto. Veamos una lista de estas palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scientific-temperature",
      "metadata": {
        "id": "scientific-temperature"
      },
      "outputs": [],
      "source": [
        "print(stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed10738-1a28-4443-bab2-275b7fcc61b2",
      "metadata": {
        "id": "7ed10738-1a28-4443-bab2-275b7fcc61b2"
      },
      "source": [
        "La eliminación de palabras de parada no siempre es apropiada en las tareas de procesamiento de lenguaje natural. Por ejemplo, si queremos construir un modelo que genere texto coherente es necesario preservar la estructura sintáctica del lenguaje y esto incluye por supuesto las palabras de parada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab51ba1-f40f-4dfc-9d07-64e52b8b22c5",
      "metadata": {
        "id": "9ab51ba1-f40f-4dfc-9d07-64e52b8b22c5"
      },
      "source": [
        "## 2. Representación del texto\n",
        "\n",
        "En esta etapa debemos tomar el texto procesado y representarlo de alguna forma que nos permita operarlo apropiadamente. Vamos a usar la representación más básica que es la *bolsa de palabras*.  Una bolsa de palabras es una representación de texto que describe la ocurrencia de palabras dentro de un documento e implica:\n",
        "\n",
        "- Un vocabulario de palabras.\n",
        "- Una ponderación de la presencia de palabras del vocabulario. La forma mas simple es realizar el conteo de las ocurrecias de las palabras del vocabulario en el texto.\n",
        "\n",
        "Se llama *bolsa de palabras*, porque se descarta cualquier información sobre el orden o la estructura de las palabras en el documento. El modelo solo se preocupa por si las palabras conocidas ocurren en el documento o no. Para construir una bolsa de palabras en sklearn podemos hacer uso de [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) que construye el vocabulario de palabras de nuestro conjunto de datos y transforma el texto en un vector de conteo de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aboriginal-brain",
      "metadata": {
        "id": "aboriginal-brain"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=2500, stop_words=stopwords.words('spanish'))\n",
        "# max_features representa el tamaño del vocabulario. Vamos a permitir 2500 palabras.\n",
        "# stop_words le indicamos las palabras de parada para que las ignore en el vocabulario.\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3659ac1-9615-432d-b08e-18b9711fa13c",
      "metadata": {
        "id": "f3659ac1-9615-432d-b08e-18b9711fa13c"
      },
      "source": [
        "#### 2.1. Vocabulario\n",
        "\n",
        "Revisemos el vocabulario. *CountVectorizer* tomo todas las críticas del dataset, extrajo todas las palabras diferentes y calculo el número de apariciones. De todas las palabras matuvo las 2500 mas frecuentes sin tener en cuenta la lista de palabras de parada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "living-counter",
      "metadata": {
        "id": "living-counter"
      },
      "outputs": [],
      "source": [
        "# Para ver las primeras 10 palabras del vocabulario podemos hacer uso del método get_feature_names\n",
        "vectorizer.get_feature_names_out()[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6cbec6",
      "metadata": {},
      "outputs": [],
      "source": [
        "texto_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f81321-bc57-4204-9acb-76c42b082d7e",
      "metadata": {
        "id": "c9f81321-bc57-4204-9acb-76c42b082d7e"
      },
      "source": [
        "#### 2.2. Representación vectorial del texto\n",
        "\n",
        "Una vez construido el vocabulario, *CountVectorizer* construye una representación vectorial de cada crítica. El valor de cada dimensión de dicho vector corresponde al número  de ocurrencias de una palabra del vocabulario. Este vector tendra po lo tanto tantas dimensiones como palabras en el vocabulario, en nuestro caso 2500.\n",
        "\n",
        "Volvamos al texto procesado de la crítica 2895:\n",
        "\n",
        "*\"no pud acab de verl me aburr no le vi la graci ocho apell vasc much men se la voy ver este desgraci efect secundari el cas es que empec verl con gan ver si por lo men me rei de algun chist per no los distingu no exist en el text ando espesit yo el guion es un absurd sin graci porqu hay absurd gracios per no este si no hay guion no hay peli por much que teng karr a la sard me los imagin los dos compart unas kokotx con xamfain hac confident acerc de lo que tien que hac uno par que le qued una pension decent en realid esta es la critic de las tres cuart part de la pelicul el final me cog hac el colaca par irme la cam\n",
        "\"*\n",
        "\n",
        "Como se pueden dar cuenta la palabra *absurd* aparece dos veces en el texto. Esta palabra es la número 9 en nuestro vocabulario, por lo tanto la representación vectorial de la crítica 2895 deberá contener un 2 (frecuencia) en la dimension 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4937f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df864ae3",
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce48a480",
      "metadata": {},
      "outputs": [],
      "source": [
        "texto_procesado[2895]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "level-applicant",
      "metadata": {
        "id": "level-applicant"
      },
      "outputs": [],
      "source": [
        "print(len(texto_features[2895])) # Longitud del vector que representa a la crítica 2895.\n",
        "print(texto_features[2895][0:100]) # 100 primeras posiciones del vector de la crítica. Puede ubicar la dimensión correspondiente a la palabra \"absurb\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3875be-6773-4042-b55f-37c4ad1d45ec",
      "metadata": {
        "id": "1b3875be-6773-4042-b55f-37c4ad1d45ec"
      },
      "source": [
        "## 3. Modelamiento y evaluación\n",
        "\n",
        "Ahora que tenemos nuestra representación, es necesario construir y evaluar nuestro modelo de Naive Bayes. Necesitamos un subconjunto de los datos para construir nuestro modelo probabilisitico (train) y otro subconjunto para evaluarlo (test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mineral-announcement",
      "metadata": {
        "id": "mineral-announcement"
      },
      "outputs": [],
      "source": [
        "# Aca dividimos nuestro dataset en entrenamiento y texto. 20% para evaluar (test) y 80% para entrenamiento (train).\n",
        "# En las variables X_ quedaran almacenados las presentaciones vectoriales de las críticas.\n",
        "# En las variables y_ las etiquetas o categorias (polaridad de la crítica).\n",
        "X_train, X_test, y_train, y_test = train_test_split(texto_features, labels, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380a64d8-8133-4761-8972-8ee83e0eccf9",
      "metadata": {
        "id": "380a64d8-8133-4761-8972-8ee83e0eccf9"
      },
      "source": [
        "Ahora vamos a entrenar el modelo. En sklearn [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB) nos permite entrenar un modelo de Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "owned-minute",
      "metadata": {
        "id": "owned-minute"
      },
      "outputs": [],
      "source": [
        "# Ahora entrenemos un modelo simple. Ya conocenos Naive Bayes!!\n",
        "nb = MultinomialNB()\n",
        "# El método fit en sklearn permite ejecutar el proceso de entrenamiento.\n",
        "nb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192c5ae5-19f5-4351-a83f-f4200ac8cce9",
      "metadata": {
        "id": "192c5ae5-19f5-4351-a83f-f4200ac8cce9"
      },
      "source": [
        "#### 3.1. Evaluación\n",
        "\n",
        "Listo, ya tenemos nuesto modelo de Naive Bayes entrenado, es necesario evaluarlo con el cunjunto de test. Vamos a utilizar una metrica conocida como accuracy que representa la relación de críticas para las cuales el modelo predijo correctamente la crítica. Un valor de 1 en el accuracy indica que a todas las criticas se les predijo correctamente la polaridad, mientras que un accuracy de 0 indica que el modelo fallo todas sus predicciones. Existen otras métricas de evaluación pero por ahora lo vamos a mantener simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "roman-wrapping",
      "metadata": {
        "id": "roman-wrapping"
      },
      "outputs": [],
      "source": [
        "# Ahora vamos a evaluar que tan bueno es nuestro modelo NB, utlilizando el conjunto de datos de test.\n",
        "# Para predecir utlizamos el método predict.\n",
        "\n",
        "predictions = nb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNP5nJ6n-HXM",
      "metadata": {
        "id": "MNP5nJ6n-HXM"
      },
      "outputs": [],
      "source": [
        "# Ahora calculamos el score de accuracy enviando las predicciónes y los valores reales de polaridad.\n",
        "print(accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4c3f5a-6f31-4b4d-b65a-150ec6408203",
      "metadata": {
        "id": "fe4c3f5a-6f31-4b4d-b65a-150ec6408203"
      },
      "source": [
        "Un accuracy mayor al 80% no esta nada mal para un modelo simple como Naive Bayes. Vamos ahora a jugar un poco con el modelo y enviarle texto que nosotros construyamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incomplete-bible",
      "metadata": {
        "id": "incomplete-bible"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Utilizemos la siguiente frase de test (sientanse en la libertad de cambiarla).\n",
        "test = \"Fue la mejor pelicula Pelicula que he visto en mi vida\"\n",
        "\n",
        "# Recuerden el modelo recibe la representación vectorial del texto, no el texto en bruto. Debemos procesar y representar el texto.\n",
        "\n",
        "test_procesado = processing_text(test) # Aplicamos nuestra función de procesamiento\n",
        "print(\"PASO 1 procesamiento:\" ,test_procesado)\n",
        "\n",
        "test_bow =vectorizer.transform([test_procesado]) # Ahora lo representamos como una bolsa de palabras. El vector resultante tiene 2500 posiciones.\n",
        "print(\"PASO 2 representación:\" ,test_bow) # En esa impresion no aparecen las 2500 posiciones del vector, solo la lista de posiciones que son diferentes de cero.\n",
        "\n",
        "# Ahora que ya lo tenemos en la representación adecuada lo podemos enviar al modelo para que este prediga su polaridad.\n",
        "clase_test = nb.predict(test_bow)\n",
        "print(\"PASO 3 predecir con el modelo:\" ,clase_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b132dbb7-0d43-48e0-a701-2829292c2a17",
      "metadata": {
        "id": "b132dbb7-0d43-48e0-a701-2829292c2a17"
      },
      "source": [
        "Modifique el texto en la variable test para que el modelo ahora prediga una polaridad negativa. ¿Se le ocurre alguna forma de confundir al modelo para que la predicción sea incorrecta?, pruebe por ejemplo doble negaciones en el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ioGv9tsCHEj",
      "metadata": {
        "id": "0ioGv9tsCHEj"
      },
      "outputs": [],
      "source": [
        "## OPCIONAL\n",
        "## Vamos a profundizar en estos conceptos mas adelante.\n",
        "cm = confusion_matrix(y_test, predictions,labels=nb.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6Y2RjDiIiPS",
      "metadata": {
        "id": "q6Y2RjDiIiPS"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb.classes_)\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4goM7aR4CO9i",
      "metadata": {
        "id": "4goM7aR4CO9i"
      },
      "outputs": [],
      "source": [
        "## OPCIONAL\n",
        "print(classification_report(y_test, predictions, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
