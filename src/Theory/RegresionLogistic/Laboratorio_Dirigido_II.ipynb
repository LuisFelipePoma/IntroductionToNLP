{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnwuhv78GDd"
      },
      "source": [
        "#  Laboratorio #2: Regresión logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KolWC8COTmdt"
      },
      "source": [
        "# Regresión logística Binaria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00WGFBhAUX4g"
      },
      "source": [
        "## Función de activación sigmoidal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XND9mx9jWETi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnkGDtkoVY45"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "La función sigmoidal nos permite expresar el valor de z entre 0 y 1 usando el\n",
        "simil de una probabilidad condicional.\n",
        "En este caso z sería la ecuación de la regresión W*X + b\n",
        "'''\n",
        "def sigmoid(z):\n",
        "  '''\n",
        "  z = w*X + b\n",
        "  w: es el vector de pesos o de coeficientes\n",
        "  x: vector de features\n",
        "  b: El interceto, también conocido como bias\n",
        "  '''\n",
        "  sgm = 1.0 / (1.0 + np.exp(-z))\n",
        "  return sgm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoL4CfE2wBiJ"
      },
      "outputs": [],
      "source": [
        "#grafica de la función sigmoidal\n",
        "z = np.arange(-7, 7, 0.1)\n",
        "sigmoidal_z = sigmoid(z)\n",
        "\n",
        "plt.plot(z, sigmoidal_z)\n",
        "plt.yticks([0.0, 0.5, 1.0])\n",
        "plt.axvline(0.0, color='k')\n",
        "ax = plt.gca()\n",
        "ax.yaxis.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adpayDnqUZ8i"
      },
      "source": [
        "## Función de perdida (Cross-entropy loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOm7G-mFX5It"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "La función de perdida (loss) nos permite estimar la distancia que hay entre los valores\n",
        "reales de la variable dependiente (y) y los valores predichos(y^). Nuestro\n",
        "principal objetivo es desplazar los valores de w(coeficientes o pesos) y b(intercepto o\n",
        "bias) para ir minimizando iterativamente la distancia entre la variable dependiente y los valores\n",
        "predichos.\n",
        "\n",
        "Esta función para regresión logística es convexa, es decir, solo tienen un mínimo\n",
        "'''\n",
        "\n",
        "def loss_function(y_real, y_pred):\n",
        "  '''\n",
        "  y_real: los valores reales obtenidos en el dataset\n",
        "  y_pred: los valores que se encontraron a partir de la función sigmoide\n",
        "\n",
        "  '''\n",
        "  cross_entropy_loss = -(y_real*np.log(y_pred) + (1-y_real)*np.log(1-y_pred))\n",
        "\n",
        "  return cross_entropy_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2XSjBAbUaWc"
      },
      "source": [
        "## Gradiente descendiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp_-mwi-rJgg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Una vez encontrada la perdida con la función de coste necesitamos un algoritmo\n",
        "de optimización para actualizar los valores de w y b y minimizar la perdida, para\n",
        "esto usamos el gradiente descendiente. El gradiente de una función de varias\n",
        "variables es un vector que apunta en dirección del mayor incremento en una función.\n",
        "El gradiente descendiente encuentra el gradiente de la función de perdida en el punto\n",
        "actual y se mueve en la dirección opuesta\n",
        "\n",
        "Importante: Tener cuidado con las dimensiones de las matrices a multiplicar,\n",
        "recordar que las columnas de la primera matriz deben ser iguales a las filas de la segunda\n",
        "matriz y el resultado será una matriz con el número de filas de la primera matriz y el\n",
        "número de columnas de la segunda\n",
        "'''\n",
        "def gradiente(y_real, y_pred, X):\n",
        "  n_muestras = X.shape[0]\n",
        "\n",
        "  derivada_parcial_gradiente_w =(1.0/n_muestras)/np.dot(X.T, (y_pred - y_real))\n",
        "\n",
        "  derivada_parcial_gradiente_b = (1.0/n_muestras)*np.sum((y_pred - y_real))\n",
        "\n",
        "  return derivada_parcial_gradiente_w, derivada_parcial_gradiente_b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiGKuaJFwG99"
      },
      "source": [
        "## Juntando todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Eb_YUXcwJxd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import  train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJbBuFxeoGgm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "En la práctica no se calcula gradiente por gradiente, sino que se utilizan\n",
        "batchs, los batchs son subconjuntos de los datos de m número de muestras.\n",
        "\n",
        "Epochs o epocas es el numero de iteraciones que se realizan sobre el dataset.\n",
        "\n",
        "Leraning rate o taza de aprendizaje hace referencia al paso con el que se actualizan\n",
        "los pesos y el bias, por lo general va de 0 a 1 y se cambia con potencias de 10\n",
        "'''\n",
        "\n",
        "def train(X, y, batch_size, epochs, learning_rate):\n",
        "\n",
        "  n_muestras, n_features = X.shape\n",
        "\n",
        "  #1. Inicializar los pesos, en este caso los inicializamos en cero al igual que el bias\n",
        "  w = np.zeros((n_features, 1))\n",
        "  b = 0\n",
        "\n",
        "  #2. Reshape de la entrada, para ajustarla y multiplicarla despues\n",
        "  y = y.reshape(n_muestras,1)\n",
        "\n",
        "\n",
        "  #3. perdidas\n",
        "  losses = []\n",
        "\n",
        "  #4. Comenzamos la iteracion\n",
        "  for epoch in range(epochs):\n",
        "    for i in range((n_muestras-1)//batch_size + 1): #Para que el batch no abarque mas allá del número de datos y genere un error\n",
        "\n",
        "      #Definimos el conjunto de datos donde se realizara el calculo por el batch\n",
        "      inicio = i*batch_size\n",
        "      fin = inicio+batch_size\n",
        "\n",
        "      x_batch = X[inicio:fin]\n",
        "      y_batch = y[inicio:fin]\n",
        "\n",
        "      #Calculamos la función sigmoide sobre el batch\n",
        "      y_pred = sigmoid(y_batch)\n",
        "\n",
        "      #Calculamos la función de loss sobre el batch\n",
        "      loss = loss_function(y_real=y_batch, y_pred=y_pred)\n",
        "      losses.append(loss)\n",
        "\n",
        "      #Calculamos los gradientes\n",
        "      grad_w, grad_b = gradiente(y_real=y_batch, y_pred=y_pred, X=x_batch)\n",
        "\n",
        "      #actualizamos los pesos con el learnning rate\n",
        "      w -= learning_rate*grad_w\n",
        "      b -= learning_rate*grad_b\n",
        "\n",
        "  return w, b, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NipNd1guQZy"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "  '''\n",
        "  X: Matriz con las diferentes muestras a clasificar\n",
        "  w: el vector de pesos resultante del entrenamiento\n",
        "  b: el valor de bias resultante del entrenamiento\n",
        "  '''\n",
        "\n",
        "  # calculamos las predicciones con los pesos y el bias encontrado usando la\n",
        "  # función sigmoide\n",
        "  y_pred = sigmoid(np.dot(X, w) + b)\n",
        "\n",
        "  #\n",
        "  pred_class = []\n",
        "\n",
        "  #definimos el umbral, para este caso los valores mayores a 0.5 se clasifican\n",
        "  # en la clase 1 y los menores o iguales se clasifican en la clase 0\n",
        "  for pred in y_pred:\n",
        "    if pred > 0.5:\n",
        "      pred_class.append(1)\n",
        "    else:\n",
        "      pred_class.append(0)\n",
        "\n",
        "  return np.array(pred_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jogWEKyA15mV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Función simple para observar el comportamiento del modelo, siemplemente\n",
        "contamos las predicciones correctas y las dividimos sobre el número de muestras\n",
        "'''\n",
        "def accuracy(y_real, y_pred):\n",
        "    accuracy = np.sum(y_real == y_pred) / len(y_real)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLOR4zgO4gCU"
      },
      "source": [
        "##Prueba con dataset iris\n",
        "Objetivo:Clasificar la especie de planta de acuerdo a las caracteristicas de\n",
        "sus pétalos y sépalos.\n",
        "\n",
        "descripción del dataset:\n",
        "* sepal_length: longitud del sépalo de la flor\n",
        "* sepal_width: anchura del sépalo de la flor\n",
        "* petal_length: longitud del petalo de la flor\n",
        "* petal_width: anchura del petalo de la flor\n",
        "* species: Especie a la que pertence la flor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSmtgr4rwOIa"
      },
      "outputs": [],
      "source": [
        "df = sns.load_dataset('iris')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWXXSyWgwPKM"
      },
      "outputs": [],
      "source": [
        "#Como nuestro expermiento por ahora solo funciona para clasificación binaria\n",
        "# eliminamos una de las clases\n",
        "new_df = df.loc[df['species'] != 'setosa']\n",
        "new_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdVRU3NdQaxz"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F-GQPJzI8xf"
      },
      "outputs": [],
      "source": [
        "#Se puede obersrvar que la diferencia entre las dos clases es complicada\n",
        "sns.scatterplot(x='sepal_length', y='sepal_width', data=new_df, hue='species')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8YLe1nVPXI6"
      },
      "outputs": [],
      "source": [
        "#separamos las variables independientes de la dependiente(target)\n",
        "X = new_df.drop('species', inplace=False, axis=1)\n",
        "\n",
        "y = new_df['species']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc1UdHAQQKB8"
      },
      "outputs": [],
      "source": [
        "# codificamos la variable independiente con label encoder para manejar números\n",
        "# en lugar de variables categoricas númericas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2RMkKHoQtV-"
      },
      "outputs": [],
      "source": [
        "#Creamos los conjuntos de prueba y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0MQ48RhtuLD"
      },
      "outputs": [],
      "source": [
        "x_columns_names = X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l_BqqGVtrqy"
      },
      "outputs": [],
      "source": [
        "#Estandarizamos los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train.values)\n",
        "X_test = sc.transform(X_test.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pspXQW7-t68a"
      },
      "outputs": [],
      "source": [
        "X_train = pd.DataFrame(data=X_train, columns=x_columns_names)\n",
        "X_test = pd.DataFrame(data=X_test, columns=x_columns_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-G6uvpwvLBQ"
      },
      "outputs": [],
      "source": [
        "#Obtenemos el vector de pesos, el bias y las periddas\n",
        "w, b, losses = train(X_train, y_train, 3, 20, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8t0ihyIysmo"
      },
      "outputs": [],
      "source": [
        "#predecimos los datos sobre el entrenamiento\n",
        "y_pred = predict(X_train, w, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V9sW7AV2bXZ"
      },
      "outputs": [],
      "source": [
        "#vemos como se comporto el modelo\n",
        "print(f'Accuracy para entrenamiento: {accuracy(y_train, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE9RH-eO5wvL"
      },
      "outputs": [],
      "source": [
        "#predecimos los datos sobre el test\n",
        "y_pred_test = predict(X_test, w, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yyth_Le5Z9z"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy para prueba: {accuracy(y_test, y_pred_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gsqlxZmBcsC"
      },
      "outputs": [],
      "source": [
        "#Experimentar variando el numero de epocas, el tamaño del batch y el learning_rate\n",
        "\n",
        "#Que se tiene que modificar para que sea realmente estocastico?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ1sXR-mVSH9"
      },
      "source": [
        "# Regresión Logistica usnado Sklearn\n",
        "\n",
        "Documentación:\n",
        "* LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "* SGDClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOXcNkd1Rhyp"
      },
      "source": [
        "Retomemos el ejercicio del laboratorio dirigido I."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyZSXCotABKp"
      },
      "outputs": [],
      "source": [
        "## Si la ejecución de la importación les genera error reinicien el entorno de ejecución\n",
        "## e inicie desde esta celda\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYNhoTG3ZpdS"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.SnowballStemmer('spanish') # Vamos a utlizar el Snowball Stemmer para realizar stemming (nos permite llevar las palabras a una forma estandar).\n",
        "nltk.download('stopwords') # Lista de palabras de parada en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_c9QQDWTGD0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Tutorial-NLP_Analisis_de_Sentimientos.csv', sep=',', header=0, index_col= None, engine='python',\n",
        "                 usecols=['film_name','gender','review_title','review_text','polaridad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wojQz-rETJTZ"
      },
      "outputs": [],
      "source": [
        "df['texto'] = df['review_title'] + ' ' + df['review_text']\n",
        "def processing_text(texto):\n",
        "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
        "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
        "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
        "    # Paso 5: Pasar todo el texto a minúsculas\n",
        "    processed_feature = processed_feature.lower()\n",
        "    # Paso 6: Aplicar stemming. Es una forma de enviar las palabras a una raiz común simplificando de esta manera el vocabulario.\n",
        "    #         por ejemplo las palabras (absurdo, absurdos) que estan en el review 2895 seran llevados a la raiz común \"absurd\"\n",
        "    #         y de esta forma se evita tener dos palabras diferentes con el mismo significado en nuestro vocabulario.\n",
        "    processed_feature = \" \".join([stemmer.stem(i) for i in processed_feature.split()])\n",
        "\n",
        "\n",
        "    return processed_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2b4wxajTQE3"
      },
      "outputs": [],
      "source": [
        "texto_para_procesar = df['texto'].values\n",
        "labels = df['polaridad'].values\n",
        "\n",
        "# El texto ya procesado de cada ejemplo en nuestro dataset lo almacenaremos en la variable \"texto_procesado\"\n",
        "texto_procesado = []\n",
        "for sentence in range(0, len(texto_para_procesar)):\n",
        "    procesado = processing_text(texto_para_procesar[sentence])\n",
        "    texto_procesado.append(procesado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhE5KIGZTY5o"
      },
      "outputs": [],
      "source": [
        "print(\"Sin procesar:\")\n",
        "print(texto_para_procesar[2895])\n",
        "print(\"---------------------------------\")\n",
        "print(\"Procesado:\")\n",
        "print(texto_procesado[2895])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ1A1RZxTd5U"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=2500, stop_words=stopwords.words('spanish'))\n",
        "# max_features representa el tamaño del vocabulario. Vamos a permitir 2500 palabras.\n",
        "# stop_words le indicamos las palabras de parada para que las ignore en el vocabulario.\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zQ7dgJJTih-"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVP1pNLHTl4r"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(texto_features, labels, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZIiC8Zh7lUd"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgcY3C62vcGg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Vamos a probar los dos algoritmos de regresión logistica que tiene sklearn:\n",
        "LogisticRegression y SGDClassifier\n",
        "\n",
        "'''\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2U2fkN7SXp"
      },
      "source": [
        "### SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZG17WWmvcwk"
      },
      "outputs": [],
      "source": [
        "logistic_model_SGD = SGDClassifier(loss='log_loss',learning_rate='constant',eta0=0.1 ) # investicar los parámetros en la documentacion y variar el learning_rate\n",
        "logistic_model_SGD.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ejg2lolwcW4"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f'Clases de la variable dependiente: {logistic_model_SGD.classes_}')\n",
        "print('\\n')\n",
        "print('Vectores de coeficientes:')\n",
        "print(logistic_model_SGD.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7BVyDLGw60t"
      },
      "outputs": [],
      "source": [
        "y_pred = logistic_model_SGD.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9qR_T5Xxt6q"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred)}')\n",
        "print('Matriz de confusión:')\n",
        "matriz_confusion = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion, display_labels=logistic_model_SGD.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTs5AvdbxIkz"
      },
      "outputs": [],
      "source": [
        "y_pred_test = logistic_model_SGD.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvo7U8ZIxNAl"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test)}')\n",
        "print('Matriz de confusión:')\n",
        "matriz_confusion_test = confusion_matrix(y_test, y_pred_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test, display_labels=logistic_model_SGD.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQf3I_ed7Yiq"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdBeFNMDvkzT"
      },
      "outputs": [],
      "source": [
        "## El modelo de LogisticRegression en sklearn no ultiliza SGD!\n",
        "## Utiliza otro método denominado SAG solver (Stochastic Average Gradient)\n",
        "\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "logistic_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk5eGjT6wT9_"
      },
      "outputs": [],
      "source": [
        "print(f'Clases de la variable dependiente: {logistic_model.classes_}')\n",
        "print('\\n')\n",
        "print('Vectores de coeficientes:')\n",
        "print(logistic_model.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyuTnOMwxrf"
      },
      "outputs": [],
      "source": [
        "y_pred_2 = logistic_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCsHC0F4z5iA"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred_2)}')\n",
        "print('Matriz de confusión:')\n",
        "matriz_confusion_2 = confusion_matrix(y_train, y_pred_2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_2, display_labels=logistic_model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KzIST0IxPxS"
      },
      "outputs": [],
      "source": [
        "y_pred_test_2 = logistic_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEyZO_w40UiA"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test_2)}')\n",
        "print('Matriz de confusión:')\n",
        "matriz_confusion_test_2 = confusion_matrix(y_test, y_pred_test_2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test_2, display_labels=logistic_model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KolWC8COTmdt",
        "00WGFBhAUX4g",
        "adpayDnqUZ8i",
        "M2XSjBAbUaWc",
        "hiGKuaJFwG99",
        "fLOR4zgO4gCU",
        "pQ1sXR-mVSH9",
        "pjH7e2VC097M",
        "XKkIQwdAidNT",
        "sIZ1_TFujd-A",
        "IZIiC8Zh7lUd",
        "kx2U2fkN7SXp",
        "AQf3I_ed7Yiq"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
