{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxDu4x0GVRqE"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urxfhlehVNVI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Librerias para graficación\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IwbdFalVU_W"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJIEglbO-00M"
      },
      "source": [
        "## Lectura del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJKEU4Oz-0jO",
        "outputId": "d5822f41-5eff-48d4-c0a9-94e4d8a9ace6"
      },
      "outputs": [],
      "source": [
        "# Descargar archivos parquet y guardarlos localmente\n",
        "# !python -m wget \"https://huggingface.co/api/datasets/amazon_us_reviews/parquet/Video_Games_v1_00/train/0.parquet\" -o 0.parquet\n",
        "# !python -m wget \"https://huggingface.co/api/datasets/amazon_us_reviews/parquet/Video_Games_v1_00/train/1.parquet\" -o 1.parquet\n",
        "# !python -m wget \"https://huggingface.co/api/datasets/amazon_us_reviews/parquet/Video_Games_v1_00/train/2.parquet\" -o 2.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKQ1-fU8VUhJ"
      },
      "outputs": [],
      "source": [
        "df0 = pd.read_parquet(\"0.parquet\")\n",
        "df1 = pd.read_parquet(\"1.parquet\")\n",
        "df2 = pd.read_parquet(\"2.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9yRDyJ2VUkA"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df0, df1, df2], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "OFNAjYOnHZBR",
        "outputId": "811eefcc-1f6e-4b3f-99d7-077f865f5b55"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='star_rating', kind='count', color='r', data=df)\n",
        "plt.title('Distribución de Ejemplos')\n",
        "plt.xlabel('star_rating')\n",
        "plt.ylabel('Conteo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LpC0G1SAJM5"
      },
      "source": [
        "## Sample del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e--1CnbwCMDm"
      },
      "outputs": [],
      "source": [
        "def balancear_dataset_por_rating(data_frame):\n",
        "    # Obtenemos el recuento de cada valor de \"rating\"\n",
        "    conteo_ratings = data_frame['star_rating'].value_counts()\n",
        "\n",
        "    # Obtenemos el mínimo de ocurrencias de \"rating\" que queremos mantener\n",
        "    min_ocurrencias = min(conteo_ratings)\n",
        "\n",
        "    # Filtramos las filas que tienen \"rating\" mayor o igual al mínimo especificado\n",
        "    data_frame_balanceado = data_frame.groupby('star_rating').apply(lambda x: x.sample(min_ocurrencias, random_state=42))\n",
        "\n",
        "    # Restablecemos el índice del DataFrame resultante y eliminamos el índice antiguo\n",
        "    data_frame_balanceado.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return data_frame_balanceado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQlPBS21VUmU",
        "outputId": "ed95ddd5-7da2-4f05-ab57-457bc4162e17"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQWS0JY0CTjQ"
      },
      "outputs": [],
      "source": [
        "df_balanceado = balancear_dataset_por_rating(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaA4_Xu1Aig1"
      },
      "outputs": [],
      "source": [
        "df_train = df_balanceado.copy()\n",
        "df_train['text'] = df_train['review_headline'] + ' ' + df_train['review_body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4yzKomHRfzV"
      },
      "outputs": [],
      "source": [
        "# Se obtiene una muestra del dataset\n",
        "df_train = df_train.sample(250000,random_state=42)\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_train = df_train[[\"text\", \"star_rating\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.catplot(x='star_rating', kind='count', color='r', data=df_train)\n",
        "plt.title('Distribución de Ejemplos')\n",
        "plt.xlabel('star_rating')\n",
        "plt.ylabel('Conteo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vghh3UxRAmky"
      },
      "source": [
        "# Modelamiento por Arquitecturas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qpHwvPYAqYF"
      },
      "outputs": [],
      "source": [
        "def processing_text(texto):\n",
        "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
        "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
        "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
        "    # Paso 5: Pasar todo el texto a minúsculas\n",
        "    processed_feature = processed_feature.lower()\n",
        "    # Paso 6: Aplicar stemming. Es una forma de enviar las palabras a una raiz común simplificando de esta manera el vocabulario.\n",
        "    #         por ejemplo las palabras (absurdo, absurdos) que estan en el review 2895 seran llevados a la raiz común \"absurd\"\n",
        "    #         y de esta forma se evita tener dos palabras diferentes con el mismo significado en nuestro vocabulario.\n",
        "    processed_feature = \" \".join([stemmer.stem(i) for i in processed_feature.split()])\n",
        "\n",
        "\n",
        "    return processed_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdZAH3K0As6H"
      },
      "outputs": [],
      "source": [
        "# texto_para_procesar y labels respectivamente\n",
        "texto_para_procesar = df_train['text'].values\n",
        "labels = df_train['star_rating'].values\n",
        "\n",
        "# El texto ya procesado de cada ejemplo en nuestro dataset lo almacenaremos en la variable \"texto_procesado\"\n",
        "texto_procesado = []\n",
        "for sentence in range(0, len(texto_para_procesar)):\n",
        "    procesado = processing_text(texto_para_procesar[sentence])\n",
        "    texto_procesado.append(procesado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def processing_text_for_embedding(texto):\n",
        "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
        "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
        "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
        "    # Paso 5: Pasar todo el texto a minúsculas\n",
        "    processed_feature = processed_feature.lower()\n",
        "    return processed_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# El texto ya procesado de cada ejemplo en nuestro dataset lo almacenaremos en la variable \"texto_procesado\"\n",
        "embedding = []\n",
        "for sentence in range(0, len(texto_para_procesar)):\n",
        "    procesado = processing_text_for_embedding(texto_para_procesar[sentence])\n",
        "    embedding.append(procesado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo Word2Vec de Google News\n",
        "word2vec_model_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para convertir el texto procesado en vectores de Word2Vec promediando los vectores de las palabras presentes\n",
        "def word2vec_features(texto_procesado, word2vec_model, vector_size=300):\n",
        "    texto_vectors = []\n",
        "    for sentence in texto_procesado:\n",
        "        vectors = []\n",
        "        for word in sentence:\n",
        "            if word in word2vec_model:\n",
        "                vectors.append(word2vec_model[word])\n",
        "        if vectors:\n",
        "            texto_vectors.append(np.mean(vectors, axis=0))\n",
        "        else:\n",
        "            texto_vectors.append(np.zeros(vector_size))\n",
        "    return np.array(texto_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ahora creamos los vectores de características utilizando Word2Vec\n",
        "texto_embedding = word2vec_features(texto_procesado, word2vec_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0HEHwpAnGP"
      },
      "source": [
        "## Naives Bayes (MultinomialNB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ1zYqZcAo94",
        "outputId": "9bdf0e63-2d97-41ae-fe15-6923cb787494"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO2HOCvvIFHv"
      },
      "outputs": [],
      "source": [
        "def create_model_naives(texto_features):\n",
        "    # Partición del dataset: Seleccionar 80% para entrenamiento, 20% pruebas.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texto_features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Modelo: Naive Bayes\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Reporte de clasificación\n",
        "    predictions = model.predict(X_test)\n",
        "    print(classification_report(y_test, predictions, digits=4))\n",
        "\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(y_test, predictions,labels=model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "    disp.plot()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncHBHKciAuZN"
      },
      "source": [
        "### Representacion 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "LqwWL_kiDThh",
        "outputId": "fec0c117-b768-4f6f-ed14-4d57363e2990"
      },
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=2500, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_naives(texto_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh6ABfJuAvo-"
      },
      "source": [
        "### Representacion 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_naives(texto_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzUzefj6AyNZ"
      },
      "source": [
        "### Representacion 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=6500, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_naives(texto_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd_IP0obA9uR"
      },
      "source": [
        "## Logistic Regresion (SDGClassfier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPelrIJ_A_Kb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_sdg(texto_features, eta = 0.1):\n",
        "    # Partición del dataset: Seleccionar 80% para entrenamiento, 20% pruebas.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        texto_features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Modelo: Naive Bayes\n",
        "    # investicar los parámetros en la documentacion y variar el learning_rate\n",
        "    model = SGDClassifier(loss='log_loss', learning_rate='constant', eta0=eta)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Reporte de clasificación\n",
        "    predictions = model.predict(X_test)\n",
        "    print(classification_report(y_test, predictions, digits=4))\n",
        "\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm, display_labels=model.classes_)\n",
        "    disp.plot()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3BBGWkZA-R6"
      },
      "source": [
        "### Representacion 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=2500, stop_words=stopwords.words('english'))\n",
        "# max_features representa el tamaño del vocabulario. Vamos a permitir 2500 palabras.\n",
        "# stop_words le indicamos las palabras de parada para que las ignore en el vocabulario.\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_sdg(texto_features,eta=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm24VD7zBA88"
      },
      "source": [
        "### Representacion 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=4500, stop_words=stopwords.words('english'))\n",
        "# max_features representa el tamaño del vocabulario. Vamos a permitir 2500 palabras.\n",
        "# stop_words le indicamos las palabras de parada para que las ignore en el vocabulario.\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_sdg(texto_features,eta=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vnhGYFABBOQ"
      },
      "source": [
        "### Representacion 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=6500, stop_words=stopwords.words('english'))\n",
        "# max_features representa el tamaño del vocabulario. Vamos a permitir 2500 palabras.\n",
        "# stop_words le indicamos las palabras de parada para que las ignore en el vocabulario.\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(texto_procesado).toarray().astype(\"float16\")\n",
        "\n",
        "# Creamos el modelo y sus metricas\n",
        "create_model_sdg(texto_features,eta=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPObYKD7BDEO"
      },
      "source": [
        "## Neuronal Network (Sequencial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6A-TEy1BEv1"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7J2yXrKBHcK"
      },
      "source": [
        "### Representacion 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RruJ4O_rBIBh"
      },
      "source": [
        "### Representacion 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACWBFlEkBIST"
      },
      "source": [
        "### Representacion 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
